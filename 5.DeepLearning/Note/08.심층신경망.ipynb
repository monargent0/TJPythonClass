{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer 2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "(train_input, train_target) , (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/ojeongeun/GitHub/Python/5.DeepLearning/Note/08.심층신경망.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ojeongeun/GitHub/Python/5.DeepLearning/Note/08.%EC%8B%AC%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39m# train과 valid 데이터 분리\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ojeongeun/GitHub/Python/5.DeepLearning/Note/08.%EC%8B%AC%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ojeongeun/GitHub/Python/5.DeepLearning/Note/08.%EC%8B%AC%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb#ch0000003?line=3'>4</a>\u001b[0m train_scaled \u001b[39m=\u001b[39m train_input \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ojeongeun/GitHub/Python/5.DeepLearning/Note/08.%EC%8B%AC%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb#ch0000003?line=4'>5</a>\u001b[0m train_scaled \u001b[39m=\u001b[39m train_scaled\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m) \u001b[39m# 2차원 배열로 변환하는것은 현업, CNN에서는 하지 않는다. 배우는 과정이라서 하는것\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# train과 valid 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_scaled = train_input / 255.0\n",
    "train_scaled = train_scaled.reshape(-1, 28*28) # 2차원 배열로 변환하는것은 현업, CNN에서는 하지 않는다. 배우는 과정이라서 하는것\n",
    "\n",
    "train_scaled , val_scaled, train_target, val_target = train_test_split(\n",
    "    train_scaled , train_target , test_size= 0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 딥러닝은 모델을 만들기 전 설계서를 만들어 보면서 해야한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer를 추가하는 방법 - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 만들기\n",
    "dense1 = keras.layers.Dense(100 , activation = 'sigmoid', input_shape = (784,)) # 은닉층\n",
    "dense2 = keras.layers.Dense(10 , activation = 'softmax') # 출력층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 14:36:00.415283: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 전체 모델에 층을 추가\n",
    "model = keras.Sequential([dense1 , dense2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary (Layer 확인)\n",
    "model.summary() \n",
    "# Param : 가중치 갯수 \n",
    "# 784*100 (w) , 100 (b) = 78500 \n",
    "# Param갯수가 줄어 나가야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Layer를 추가하는 방법 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(100, activation = \"sigmoid\" , input_shape = (784,) , name = 'hidden'),\n",
    "        keras.layers.Dense(10, activation = \"softmax\" , name = 'output')\n",
    "    ], name = \"패션 Mnist모델\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"패션 Mnist모델\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Layer를 추가하는 방법 - 3\n",
    "주로 사용하는 방법      \n",
    "데이터마다 사용할 레이어 갯수가 다를 수 있기 때문에 if문으로 add를 조절할 수 있다. (-> 알파고)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(100 , activation = \"sigmoid\" , input_shape = (784,)))\n",
    "model.add(keras.layers.Dense(10 , activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.5654 - accuracy: 0.8081\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4069 - accuracy: 0.8526\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3720 - accuracy: 0.8652\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3499 - accuracy: 0.8737\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3320 - accuracy: 0.8794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1acf0fc10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', metrics = 'accuracy')\n",
    "model.fit(train_scaled , train_target , epochs= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid / Relu\n",
    "- Sigmoid 함수는 숫자가 무한대로 커지게 되면 데이터의 차이가 거의 없다는 단점이 있다.\n",
    "- 이를 해결한 것이 Relu 함수 인데 인공신경망에 기여한 바가 크며 음수는 모든 0으로 양수는 항상 최대값을 출력하는 함수 \n",
    "\n",
    "sigmoid는 0.5기준으로는 구분이 잘 되지만 숫자가 커지는 경우가 애매해진다. 0.999999999 0.999999999999999 등 구분이 안 된다.        \n",
    "relu는 음수는 없고 양수로 계속 커지는 함수이므로 sigmoid의 단점을 해결해준다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력층을 단순화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력층을 단순화 하게 \n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = (28,28))) # 입력층을 2차원 원래대로 사용한다. -> 알아서 펼쳐줌, 제일 편함 \n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimizer(최적화 알고리즘)\n",
    "- 훈련 시에 Optimizer를 사용하여 최적화 알고리즘을 구성한다.\n",
    "- 보통 사용하는 것이 확률적 경사 하강법 optimizer이다. (running rate, Jump를 얼마나 할까)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 적용 방법 (확률적 경사 하강법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 방법 (확률적 경사 하강법) - 1\n",
    "model.compile(optimizer ='sgd' , loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 방법 (확률적 경사 하강법) - 2\n",
    "sgd = keras.optimizers.SGD()\n",
    "model.compile(optimizer=sgd, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 방법 (확률적 경사 하강법) + 학습률 변경 - 3 : sgd의 기본값 learning_rate = 0.01\n",
    "sgd = keras.optimizers.SGD(learning_rate= 0.1) # 기본값보다 10배정도 더 빠르게 움직일거야\n",
    "model.compile(optimizer=sgd, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용방법 (Momentum)\n",
    "sgd = keras.optimizers.SGD(momentum= 0.9 , nesterov= True) # 기본값 0.9\n",
    "model.compile(optimizer=sgd, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
    "# 아담에 모멘텀 방식이 적용되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용방법 (Adagrad)\n",
    "adagrad = keras.optimizers.Adagrad()\n",
    "model.compile(optimizer=adagrad, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용방법 (RMSprop)\n",
    "rmsprop = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer=rmsprop, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용 방법\n",
    "model.compile(optimizer ='rmsprop' , loss = 'sparse_categorical_crossentropy', metrics = 'accuracy') # 위의 방법도 있지만 그냥 이렇게 쓰는게 편함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ada30cce870f40109c20350bb1c1fbd35f5f6350419973ff2738af6ed49d045e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
